Wikipedia
---------

List of languages was extracted from http://meta.wikimedia.org/wiki/List_of_Wikipedias into ~/Data/Wikipedia/LANGUAGE_NAMES.txt. Since we could work with only those languages which had data in WALS, we keep only those languages which are in WALS (~/Data/Wikipedia/languages.csv available from WALS), along with their 3 letter codes. This filtering is done by ~/Data/Wikipedia/getcodes.py. The languages and their codes are dumped into "codes_with_languages.txt". Only the language codes are available in "only_codes.txt"

Language data was collected from two sources: WALS and ASJP.

Data from WALS
--------------

WALS Data is available as a bunch of CSV files - datapoints.csv, features.csv, languages.csv and values.csv. These are bundled in ~/Data/WALS/wals-excel-utf8-2013-11-05.zip. A README file is also included which gives details of these files. "datapoints.csv" contains the feature matrix for all WALS Languages. Since we are interested only in those present in Wikipedia, we use "matrixcreate.py" to filter out the wikipedia languages. Lines 21-28 (which are now commented out) did the job. The data is dumped in "datapoints_wikipedia.csv". To use only the most stable features, we do further filtering of "datapoints_wikipedia.csv". This is done by lines 31-47 of "matrixcreate.py". The new matrix is dumped in "datapoints_wikipedia_msf.csv". As we will see later since the ASJP World Language Tree with which we compare our tree does not contain all the langaues in "datapoints_wikipedia_msf.csv", we get the subset of languages present in the ASJP World Language Tree into "NewList.txt". To create the input file for PHYLIP's PARS program, we use "create_input_phylip.py" which reads data from "datapoints_wikipedia_msf.csv" and "NewList.txt" to generate the input file. Only languages which have more than 10 features are considered. This results in selecting 122 languages. "in_phylip_gt10.txt" contains the dump of "create_input_phylip.py". To actually use this file with PHYLIP, you also need to add the language and feature count at the top. "create_nexus.py" creates the NEXUS file for this data, but that is required only if you are using phylogenic programs which accept input in NEXUS format.

Data from ASJP
--------------

ASJP Data is available in a big text file (~/Data/ASJP/listss16.txt). We manually filtered out the data for each language under consideration into separate files (~/Data/ASJP/LanguageFiles/<language_code>.txt). "ComputeDistance.py" reads from "in_phylip_gt10.txt" discussed under "Data from WALS" and the language's words from (~/Data/ASJP/LanguageFiles/) and generates the distance matrix, "in_fitch_gt10.txt". This is used by PHYLIP's FITCH program to compue the tree. 

Evaluation
----------

We manually create the induced subtree of languages in "datapoints_wikipedia_msf.csv" out of the ASJP World Tree. The parenthesis tree is "~/Evaluation/3_parenthesis_trees.txt". Since later we considered 122 languages of the 161 present in the parenthesis tree, we used "induced_subtree.py" to form the induced subtree of the languages in "in_fitch_gt10.txt". This program can be used on any parenthesis tree to form the subtree. "world_tree_gt10.txt" contains the data dumped by "induced_subtree.py". "compare.txt" is a single text file containing the world tree folowed by the one generated by FITCH. This is used by TREEDIST program of PHYLIP to compute the distance between trees. "compare_reduced.txt" contains the trees with lower branches cut-off. This has been done manually. This is used to find similarity at lower granularity. 

Visualization
-------------

Since the output trees contain language codes, we use "generateDrawtreeInput.py" to convert these codes to language names using "codes_with_languages.txt". "fitch_gt10_fullnames.txt" is a dump of the the output of this script. "fitch_gt10_fullnames.txt" is then fed to PHYLIP's DRAWGRAM program to generate the visual tree. 